{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81496507-f3f1-4094-bb1b-5fd2b2bc8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the application of the DNN network from (Kell et al., 2018), we want to pass the stimuli from MiND auditoty fMRI tasks to the DNN\n",
    "# and further analyze the activation characteristics in the DNN\n",
    "# written by Violet Sept.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335e9eb3-1170-4ad0-9ba8-a6b4fe275ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\quanz\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quanz\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\GLX_Project\\\\DNN\\\\kelletal2018-master\\\\kelletal2018-master\\\\network')\n",
    "from branched_network_class import branched_network\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ecf369-f3cf-4d72-98b3-01202d8862c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\kelletal2018-master\\kelletal2018-master\\pycochleagram\\erbfilter.py:8: RuntimeWarning: pycochleagram using non-interactive Agg matplotlib backend\n",
      "  from pycochleagram import utils\n"
     ]
    }
   ],
   "source": [
    "# import the following to run demo_from_wav()\n",
    "import os\n",
    "new_directory = ('C:/Users/quanz/Documents/UM/Projects/GLX_Project/DNN/kelletal2018-master/kelletal2018-master')\n",
    "os.chdir(new_directory)\n",
    "#sys.path.append(new_directory)\n",
    "from pycochleagram import cochleagram as cgram \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1c8655-898b-4899-8d91-80c860b0bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print(title+':')\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecb9f61-e276-4d41-8864-457f3f877293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pre_generated_cochleagram():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    net_object = branched_network() # make network object\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy', encoding='bytes', allow_pickle=True) #Load logits to word key \n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy',encoding='bytes', allow_pickle=True) #Load logits to genre key\n",
    "\n",
    "    # example pre-generated speech cochleagram  \n",
    "    example_cochleagram = np.load('./demo_stim/example_cochleagram_0.npy') \n",
    "    plot_cochleagram(example_cochleagram,'Example speech cochleagram' )\n",
    "\n",
    "    # run cochleagram through network and get logits for word branch\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: example_cochleagram})\n",
    "\n",
    "    # determine word branch prediction \n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print(\"Speech Example ... actual label: according  predicted_label: \" + prediction[0].decode('utf-8') +'\\n')\n",
    "    \n",
    "    # example pre-generated music cochleagram\n",
    "    example_cochleagram_music = np.load('./demo_stim/example_cochleagram_1.npy') \n",
    "    plot_cochleagram(example_cochleagram_music,'Example music cochleagram' )\n",
    "    \n",
    "    # run cochleagram through network and get logits for genre branch\n",
    "    logits_music = net_object.session.run(net_object.genre_logits, \n",
    "                                          feed_dict={net_object.x: example_cochleagram_music})\n",
    "    # note: throughout paper top-5 accuracy is reported for genre task\n",
    "    prediction_music = (logits_music).argsort()[:,-5:][0][::-1] \n",
    "    print(\"Music Example... actual label: \"+ music_key[11].decode('utf-8') + \"  top-5 predicted_labels (in order of confidence): \")\n",
    "    print(\"\\n\"+ \"; \".join(key.decode('utf-8') for key in music_key[prediction_music]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e5e757-d8fd-4887-a7cf-4b16cd53498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(wav_f, sr, title):\n",
    "    # Define parameters\n",
    "    n, sampling_rate = 50, 16000\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "\n",
    "    # Create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "    \n",
    "    # Debugging statements\n",
    "    print(\"Cochleagram shape:\", c_gram.shape)\n",
    "    print(\"Min value in cochleagram:\", np.min(c_gram))\n",
    "    print(\"Max value in cochleagram:\", np.max(c_gram))\n",
    "\n",
    "    # Avoiding log10 issues\n",
    "    # c_gram[c_gram <= 0] = 1e-10  # Set non-positive values to a small positive number\n",
    "\n",
    "    # Rescale to [0, 255]\n",
    "    # c_gram_rescaled = 255 * (1 - ((np.max(c_gram) - c_gram) / np.ptp(c_gram)))\n",
    "\n",
    "    # Check the shape of c_gram_rescaled\n",
    "    # print(\"Shape of c_gram_rescaled:\", c_gram_rescaled.shape)\n",
    "\n",
    "     # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "    \n",
    "    plot_cochleagram(c_gram_reshape_2, title)\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "    \n",
    "    return c_gram_flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02d97f4-e486-42d9-8c1d-9cf883807cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "\n",
    "def demo_from_MiND():\n",
    "    # tf.reset_default_graph()  # Reset the TensorFlow graph\n",
    "\n",
    "    global net_object\n",
    "    global c_gram\n",
    "    net_object = branched_network()  # Initialize your network model\n",
    "\n",
    "    # Load the keys for the model outputs\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')  # Speech labels\n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy')  # Music genre labels\n",
    "\n",
    "    # Replace the speech example with your Violin.wav stimulus\n",
    "    # file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\Test\\Violin.wav'\n",
    "    # sr, wav_f = wav.read('./demo_stim/example_6.wav')\n",
    "    \n",
    "    # Generate the cochleagram for Violin.wav and pass it through the network\n",
    "    # sr, wav_f = wav.read('./demo_stim/example_6.wav')\n",
    "    sr, wav_f = wav.read(r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\Test\\Word\\Segment_3.wav')\n",
    "\n",
    "    # Resample the audio to 16,000 Hz using librosa\n",
    "    # wav_f = librosa.resample(wav_f_org.astype(float), orig_sr=original_sr, target_sr=16000)\n",
    "    \n",
    "    # Check the shape and convert to mono if stereo\n",
    "    if wav_f.ndim == 2:  # Check if it's stereo\n",
    "        wav_f = np.mean(wav_f, axis=1).astype(np.int16)  # Average the two channels to create a mono signal\n",
    "\n",
    "    play_wav(wav_f, sr, 'Example')  # Play Violin.wav (if required)\n",
    "\n",
    "    # Generate cochleagram\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example')  # Assuming generate_cochleagram is defined\n",
    "\n",
    "    # Get logits for word branch (if classifying music)\n",
    "    # logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    # prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    # print(\"Music Example ... \\n Background: Auditory Scene, snr: -3db, actual label: \" \\\n",
    "    #     + music_key[1].decode('utf-8') + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "    #     + \";\\n \".join(key.decode('utf-8') for key in music_key[prediction]) + \"\\n\")\n",
    "\n",
    "    # Get logits for word branch (if classifying speech)\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print(\"Speech Example ... \\n background: speaker shaped noise, snr: -3db, actual label: Washington, predicted_label: \" \\\n",
    "        + prediction[0].decode('utf-8') +'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a43b83-08af-4b6f-bb43-f0ee0dc12aac",
   "metadata": {},
   "source": [
    "demo_from_MiND()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2f596-1d9c-46bc-85bc-ca844102d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\Test\\Violin.wav'\n",
    "sr, wav_f = wav.read(file_path)  # sr = sampling rate, wav_f = audio signal\n",
    "\n",
    "print(\"Shape of wav_f:\", wav_f.shape)\n",
    "print(\"Data type of wav_f:\", wav_f.dtype)\n",
    "print(f\"Sampling Rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa5eaf-a428-4285-be26-dd27d8357eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d6caef3-e06f-4b7a-a4da-762a9c501827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 activation shape: (1, 86, 86, 96)\n",
      "rnorm1 activation shape: (1, 86, 86, 96)\n",
      "pool1 activation shape: (1, 43, 43, 96)\n",
      "conv2 activation shape: (1, 22, 22, 256)\n",
      "rnorm2 activation shape: (1, 22, 22, 256)\n",
      "pool2 activation shape: (1, 11, 11, 256)\n",
      "conv3 activation shape: (1, 11, 11, 512)\n",
      "conv4_W activation shape: (1, 11, 11, 1024)\n",
      "conv5_W activation shape: (1, 11, 11, 512)\n",
      "pool5_W activation shape: (1, 6, 6, 512)\n",
      "conv4_G activation shape: (1, 11, 11, 1024)\n",
      "conv5_G activation shape: (1, 11, 11, 512)\n",
      "pool5_G activation shape: (1, 6, 6, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from branched_network_class import branched_network\n",
    "\n",
    "# Create an instance of the network\n",
    "net_object = branched_network()\n",
    "\n",
    "# Prepare your input data (ensure it's the correct shape)\n",
    "input_data = c_gram  # Shape should be (batch_size, 256 * 256), for example\n",
    "\n",
    "# Run the session to get activations\n",
    "activations =  net_object.get_activations(input_data)\n",
    "\n",
    "# Print the shapes of activations from each layer\n",
    "for layer_name, activation in activations.items():\n",
    "    print(f\"{layer_name} activation shape: {activation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3524badc-ca67-45b3-9074-34977d40f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example to visualize the first channel of the first layer's activation\n",
    "layer_to_visualize = 'pool5_G'  # Change to the layer you want to visualize\n",
    "activation = activations[layer_to_visualize]\n",
    "\n",
    "# Select the first filter's output (assuming a single batch size)\n",
    "plt.imshow(activation[0, :, :, 0], cmap='viridis')\n",
    "plt.title(f\"Activation from {layer_to_visualize}\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Save the figure to a file\n",
    "plt.savefig(f\"{layer_to_visualize}_activation.png\")  # Save as a PNG file\n",
    "plt.close()  # Close the figure after saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cab2e046-3cba-492c-becf-d83b9e57a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\kelletal2018-master\\kelletal2018-master\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af309d-ce43-40c1-be72-36ae813b2cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
