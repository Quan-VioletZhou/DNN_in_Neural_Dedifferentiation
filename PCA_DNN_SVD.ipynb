{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59c39f-1632-4b43-931b-e9490db1fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your activation data file\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\conv1_average_activation_matrix.csv'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = np.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "# Set the number of components to retain (e.g., 100)\n",
    "n_components = 100\n",
    "\n",
    "# Select the top components\n",
    "U_reduced = U[:, :n_components]\n",
    "S_reduced = np.diag(S[:n_components])\n",
    "Vt_reduced = Vt[:n_components, :]\n",
    "\n",
    "# Form the reduced matrix\n",
    "reduced_data = np.dot(U_reduced, S_reduced)\n",
    "\n",
    "# Save the reduced data\n",
    "output_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\conv1_svd_100_components.csv'\n",
    "pd.DataFrame(reduced_data).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SVD completed for conv1, saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476ae7c-f918-415d-ba2b-354805bc0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total variance\n",
    "total_variance = np.sum(S**2)\n",
    "\n",
    "# Calculate explained variance for the top 100 components\n",
    "n_components = 100\n",
    "explained_variance = np.sum(S[:n_components]**2)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "print(f\"Explained Variance Ratio for top {n_components} components: {explained_variance_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b972f-000c-416b-b721-1744e2b6e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'pca' is the PCA model used earlier\n",
    "# and 'kmeans' is the trained KMeans model\n",
    "# Load original activation matrix\n",
    "layer_file_path = '/home/tpolklabuser/Desktop/Violet/DNN/ML_RR/DNN_Activation/conv1_average_activation_matrix.csv'\n",
    "activation_matrix = np.loadtxt(layer_file_path, delimiter=',', skiprows=1)  # Shape: (710016, 12)\n",
    "\n",
    "# Step 1: Apply the same PCA transformation to the activation matrix\n",
    "activation_matrix_pca = pca.transform(activation_matrix)  # Shape: (710016, 10)\n",
    "\n",
    "# Step 2: Predict the closest centroids for each unit\n",
    "closest_centroids = kmeans.predict(activation_matrix_pca)\n",
    "\n",
    "# Step 3: Reconstruct the activation matrix using the closest centroids\n",
    "reconstructed_matrix_pca = kmeans.cluster_centers_[closest_centroids]  # Shape: (710016, 10)\n",
    "\n",
    "# Step 4: Inverse transform to get back to the original feature space\n",
    "reconstructed_matrix = pca.inverse_transform(reconstructed_matrix_pca)  # Shape: (710016, 12)\n",
    "\n",
    "# Step 5: Calculate the mean squared reconstruction error\n",
    "reconstruction_error = mean_squared_error(activation_matrix, reconstructed_matrix)\n",
    "print(f\"Reconstruction Error (MSE): {reconstruction_error:.4f}\")\n",
    "\n",
    "# Step 6: Calculate explained variance as a measure of clustering performance\n",
    "total_variance = np.var(activation_matrix)\n",
    "explained_variance_ratio = 1 - (reconstruction_error / total_variance)\n",
    "print(f\"Explained Variance by Clustering: {explained_variance_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c47616b8-1781-4763-98af-51e337c67d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced matrix shape: (80, 120)\n",
      "Explained variance ratio: 0.8551802769260989\n",
      "Singular values: [639.70775864 415.54301683 280.68413396 261.55004108 225.63190035\n",
      " 221.77132014 211.94301011 202.81815524 193.31881494 184.95515144\n",
      " 161.54688766 159.07377833 153.56660484 149.59882859 144.78090221\n",
      " 143.23613521 142.45574147 137.20341162 135.77608254 133.4640699\n",
      " 132.30314033 131.21407814 130.92991277 130.79617742 128.08121813\n",
      " 127.72939914 126.58850306 125.98338387 125.58768852 124.65807729\n",
      " 124.19321259 122.1503094  121.5848747  120.90303872 119.40566192\n",
      " 118.917725   117.46694096 116.75134808 116.12525498 115.49945305\n",
      " 115.24124712 114.56923533 114.01201535 113.3623525  112.61956307\n",
      " 112.46778952 111.47100007 111.25858944 110.61980121 109.96072399\n",
      " 109.30973278 108.08741678 107.89122056 107.13545779 106.6157933\n",
      " 106.38528047 105.23605267 104.42344982 103.75568613 103.17074599\n",
      " 102.83106847 101.85204226 101.69781394 101.37531056 100.82425433\n",
      " 100.04470097  99.93869785  99.19405773  98.13993015  97.74734316\n",
      "  96.83650138  96.36188218  95.92537995  95.53896401  95.26818918\n",
      "  94.02008885  93.64918198  92.67426064  92.42885243  91.78877806]\n"
     ]
    }
   ],
   "source": [
    "# This codes using SVD instead of PCA, according to Justine Zhang report, \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load or create your activation matrix\n",
    "file = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\pool5_G_both_combined_matrix.csv'\n",
    "activation_matrix = np.loadtxt(file, delimiter=',', skiprows=1) # Example loading step\n",
    "\n",
    "# transform to .npy for futher use\n",
    "# np.save('conv1_both_combined_matrix.npy', activation_matrix)\n",
    "\n",
    "# Define the number of components to keep (adjust based on the desired explained variance)\n",
    "n_components = 80  # Choose an appropriate number based on variance explained or experiment needs\n",
    "\n",
    "activation_matrix_centered = activation_matrix - np.mean(activation_matrix, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "activation_matrix_standardized = scaler.fit_transform(activation_matrix_centered)\n",
    "\n",
    "# Transpose the matrix to reduce along the units dimension (710061)\n",
    "activation_matrix_T = activation_matrix_standardized.T  # Now shape is [120, 710061]\n",
    "\n",
    "\n",
    "# Perform Truncated SVD\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_matrix = svd.fit_transform(activation_matrix_T)\n",
    "\n",
    "\n",
    "# Transpose back to retain the structure where rows represent units\n",
    "reduced_matrix = reduced_matrix.T  # Final shape will be [n_components, 120]\n",
    "\n",
    "# Print the shape of the reduced matrix to verify dimensions\n",
    "print(\"Reduced matrix shape:\", reduced_matrix.shape)\n",
    "\n",
    "# Print the explained variance ratio to understand how much of the original data is captured in the reduced matrix.\n",
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# Optional: Inspect the singular values, which represent the importance of each reduced dimension.\n",
    "# High singular values correlate with high variance directions, often capturing meaningful structures in the data.\n",
    "singular_values = svd.singular_values_\n",
    "print(\"Singular values:\", singular_values)\n",
    "\n",
    "# Save the reduced matrix for future analysis or model input\n",
    "# np.save('reduced_activation_matrix.npy', reduced_matrix)\n",
    "\n",
    "# np.save('C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\DNN_in_Neural_Dedifferentiation\\\\SVD_results\\\\reduced_activation_matrix_conv5_G.npy', reduced_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30f90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 120)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the reduced activation matrix from the file\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\DNN_in_Neural_Dedifferentiation\\SVD_results\\reduced_activation_matrix_rnorm1.npy'\n",
    "reduced_activation_matrix = np.load(file_path)\n",
    "\n",
    "# Print the shape of the loaded matrix\n",
    "print(reduced_activation_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will average the SVD matrix across time domain, so make the output from the previous chunck [100x120] to [100x6] condition-wise to prepare for risdge regression\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Load all .npy files\n",
    "file_paths = glob.glob(r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_activation_matrix_*.npy')  # Adjust path as needed\n",
    "activation_matrices = {file.split('\\\\')[-1].split('.')[0]: np.load(file) for file in file_paths}\n",
    "\n",
    "# Function to average every 10 columns\n",
    "def average_columns(matrix, group_size=10):\n",
    "    return matrix.reshape(matrix.shape[0], -1, group_size).mean(axis=2)\n",
    "\n",
    "# Apply column averaging to each matrix\n",
    "reduced_matrices = {}\n",
    "for name, matrix in activation_matrices.items():\n",
    "    reduced_matrix = average_columns(matrix, group_size=10)  # Shape will be (100, 12)\n",
    "    reduced_matrices[name] = reduced_matrix\n",
    "\n",
    "# Optional: Save reduced matrices if you need them for later\n",
    "for name, reduced_matrix in reduced_matrices.items():\n",
    "    np.save(f\"C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\GLX_Project\\\\DNN\\\\MiND_Stimili\\\\reduced_{name}.npy\", reduced_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00b3e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced matrix saved to: C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_reduced_activation_matrix_conv5_G.npy\n"
     ]
    }
   ],
   "source": [
    "# This code help update the average across time matrix for a specific layer\n",
    "import numpy as np\n",
    "\n",
    "# Specify the file path for conv3\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_activation_matrix_conv5_G.npy'\n",
    "\n",
    "# Load the specific file\n",
    "activation_matrix = np.load(file_path)\n",
    "\n",
    "# Function to average every 10 columns\n",
    "def average_columns(matrix, group_size=10):\n",
    "    return matrix.reshape(matrix.shape[0], -1, group_size).mean(axis=2)\n",
    "\n",
    "# Apply column averaging\n",
    "reduced_matrix = average_columns(activation_matrix, group_size=10)  # Shape will be (100, 12)\n",
    "\n",
    "# Save the reduced matrix\n",
    "output_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_reduced_activation_matrix_conv5_G.npy'\n",
    "np.save(output_path, reduced_matrix)\n",
    "\n",
    "print(f\"Reduced matrix saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bae8c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mname\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a736444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how much variance it explaines: \n",
    "total_variance_explained = svd.explained_variance_ratio_.sum()\n",
    "print(\"Total variance explained:\", total_variance_explained)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Number of Components')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
