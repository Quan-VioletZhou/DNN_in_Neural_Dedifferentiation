{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59c39f-1632-4b43-931b-e9490db1fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your activation data file\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\conv1_average_activation_matrix.csv'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = np.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "# Set the number of components to retain (e.g., 100)\n",
    "n_components = 100\n",
    "\n",
    "# Select the top components\n",
    "U_reduced = U[:, :n_components]\n",
    "S_reduced = np.diag(S[:n_components])\n",
    "Vt_reduced = Vt[:n_components, :]\n",
    "\n",
    "# Form the reduced matrix\n",
    "reduced_data = np.dot(U_reduced, S_reduced)\n",
    "\n",
    "# Save the reduced data\n",
    "output_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\conv1_svd_100_components.csv'\n",
    "pd.DataFrame(reduced_data).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SVD completed for conv1, saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476ae7c-f918-415d-ba2b-354805bc0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total variance\n",
    "total_variance = np.sum(S**2)\n",
    "\n",
    "# Calculate explained variance for the top 100 components\n",
    "n_components = 100\n",
    "explained_variance = np.sum(S[:n_components]**2)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "print(f\"Explained Variance Ratio for top {n_components} components: {explained_variance_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b972f-000c-416b-b721-1744e2b6e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'pca' is the PCA model used earlier\n",
    "# and 'kmeans' is the trained KMeans model\n",
    "# Load original activation matrix\n",
    "layer_file_path = '/home/tpolklabuser/Desktop/Violet/DNN/ML_RR/DNN_Activation/conv1_average_activation_matrix.csv'\n",
    "activation_matrix = np.loadtxt(layer_file_path, delimiter=',', skiprows=1)  # Shape: (710016, 12)\n",
    "\n",
    "# Step 1: Apply the same PCA transformation to the activation matrix\n",
    "activation_matrix_pca = pca.transform(activation_matrix)  # Shape: (710016, 10)\n",
    "\n",
    "# Step 2: Predict the closest centroids for each unit\n",
    "closest_centroids = kmeans.predict(activation_matrix_pca)\n",
    "\n",
    "# Step 3: Reconstruct the activation matrix using the closest centroids\n",
    "reconstructed_matrix_pca = kmeans.cluster_centers_[closest_centroids]  # Shape: (710016, 10)\n",
    "\n",
    "# Step 4: Inverse transform to get back to the original feature space\n",
    "reconstructed_matrix = pca.inverse_transform(reconstructed_matrix_pca)  # Shape: (710016, 12)\n",
    "\n",
    "# Step 5: Calculate the mean squared reconstruction error\n",
    "reconstruction_error = mean_squared_error(activation_matrix, reconstructed_matrix)\n",
    "print(f\"Reconstruction Error (MSE): {reconstruction_error:.4f}\")\n",
    "\n",
    "# Step 6: Calculate explained variance as a measure of clustering performance\n",
    "total_variance = np.var(activation_matrix)\n",
    "explained_variance_ratio = 1 - (reconstruction_error / total_variance)\n",
    "print(f\"Explained Variance by Clustering: {explained_variance_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47616b8-1781-4763-98af-51e337c67d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced matrix shape: (90, 120)\n",
      "Explained variance ratio: 0.875526576491034\n",
      "Singular values: [3245.24257252  958.93232008  620.28393087  469.41932077  454.65545033\n",
      "  446.5940917   437.53825277  420.88500846  402.84125898  399.47392089\n",
      "  397.45453855  383.66613994  380.2149005   376.26052622  370.11725558\n",
      "  369.03738669  367.30934737  360.08292225  353.53069113  352.5294927\n",
      "  347.85151518  345.83458232  343.18314249  338.92800378  335.34113844\n",
      "  332.95450035  330.94591394  329.62420034  324.50480316  322.13952192\n",
      "  319.30831705  315.03322686  311.064738    309.56895758  306.29250633\n",
      "  305.36092273  303.32890128  302.64255447  300.41460433  298.03028279\n",
      "  297.33561466  292.18356948  291.1143555   289.52058736  287.93567337\n",
      "  284.77125787  282.89409387  282.49132802  282.00831347  278.93800197\n",
      "  277.44486691  276.20929582  274.94715148  273.82652535  271.26426614\n",
      "  270.26298475  268.05677778  267.36127764  266.35090124  265.34048197\n",
      "  262.9089305   260.61958926  258.99714297  258.64088345  257.38227545\n",
      "  256.00809275  254.46809081  253.3554169   252.67230752  251.03513941\n",
      "  249.80654976  247.90180538  247.7890434   245.50161926  244.81910582\n",
      "  243.38241893  243.05945673  241.95497562  240.28992647  238.65591235\n",
      "  237.91211301  235.70911205  234.96950671  234.43711467  232.81395696\n",
      "  231.59488705  231.57728604  228.5641611   227.30134961  226.37533663]\n"
     ]
    }
   ],
   "source": [
    "# This codes using SVD instead of PCA, according to Justine Zhang report, \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load or create your activation matrix\n",
    "file = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\pool1_both_combined_matrix.csv'\n",
    "activation_matrix = np.loadtxt(file, delimiter=',', skiprows=1) # Example loading step\n",
    "\n",
    "# transform to .npy for futher use\n",
    "# np.save('conv1_both_combined_matrix.npy', activation_matrix)\n",
    "\n",
    "# Define the number of components to keep (adjust based on the desired explained variance)\n",
    "n_components = 90  # Choose an appropriate number based on variance explained or experiment needs\n",
    "\n",
    "activation_matrix_centered = activation_matrix - np.mean(activation_matrix, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "activation_matrix_standardized = scaler.fit_transform(activation_matrix_centered)\n",
    "\n",
    "# Transpose the matrix to reduce along the units dimension (710061)\n",
    "activation_matrix_T = activation_matrix_standardized.T  # Now shape is [120, 710061]\n",
    "\n",
    "\n",
    "# Perform Truncated SVD\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_matrix = svd.fit_transform(activation_matrix_T)\n",
    "\n",
    "\n",
    "# Transpose back to retain the structure where rows represent units\n",
    "reduced_matrix = reduced_matrix.T  # Final shape will be [n_components, 120]\n",
    "\n",
    "# Print the shape of the reduced matrix to verify dimensions\n",
    "print(\"Reduced matrix shape:\", reduced_matrix.shape)\n",
    "\n",
    "# Print the explained variance ratio to understand how much of the original data is captured in the reduced matrix.\n",
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# Optional: Inspect the singular values, which represent the importance of each reduced dimension.\n",
    "# High singular values correlate with high variance directions, often capturing meaningful structures in the data.\n",
    "singular_values = svd.singular_values_\n",
    "print(\"Singular values:\", singular_values)\n",
    "\n",
    "# Save the reduced matrix for future analysis or model input\n",
    "# np.save('reduced_activation_matrix.npy', reduced_matrix)\n",
    "\n",
    "# np.save('C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\DNN_in_Neural_Dedifferentiation\\\\SVD_results\\\\reduced_activation_matrix_rnorm1.npy', reduced_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30f90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 120)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the reduced activation matrix from the file\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\DNN_in_Neural_Dedifferentiation\\SVD_results\\reduced_activation_matrix_rnorm1.npy'\n",
    "reduced_activation_matrix = np.load(file_path)\n",
    "\n",
    "# Print the shape of the loaded matrix\n",
    "print(reduced_activation_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will average the SVD matrix across time domain, so make the output from the previous chunck [100x120] to [100x6] condition-wise to prepare for risdge regression\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Load all .npy files\n",
    "file_paths = glob.glob(r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_activation_matrix_*.npy')  # Adjust path as needed\n",
    "activation_matrices = {file.split('\\\\')[-1].split('.')[0]: np.load(file) for file in file_paths}\n",
    "\n",
    "# Function to average every 10 columns\n",
    "def average_columns(matrix, group_size=10):\n",
    "    return matrix.reshape(matrix.shape[0], -1, group_size).mean(axis=2)\n",
    "\n",
    "# Apply column averaging to each matrix\n",
    "reduced_matrices = {}\n",
    "for name, matrix in activation_matrices.items():\n",
    "    reduced_matrix = average_columns(matrix, group_size=10)  # Shape will be (100, 12)\n",
    "    reduced_matrices[name] = reduced_matrix\n",
    "\n",
    "# Optional: Save reduced matrices if you need them for later\n",
    "for name, reduced_matrix in reduced_matrices.items():\n",
    "    np.save(f\"C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\GLX_Project\\\\DNN\\\\MiND_Stimili\\\\reduced_{name}.npy\", reduced_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00b3e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced matrix saved to: C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_reduced_activation_matrix_conv5_G.npy\n"
     ]
    }
   ],
   "source": [
    "# This code help update the average across time matrix for a specific layer\n",
    "import numpy as np\n",
    "\n",
    "# Specify the file path for conv3\n",
    "file_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_activation_matrix_conv5_G.npy'\n",
    "\n",
    "# Load the specific file\n",
    "activation_matrix = np.load(file_path)\n",
    "\n",
    "# Function to average every 10 columns\n",
    "def average_columns(matrix, group_size=10):\n",
    "    return matrix.reshape(matrix.shape[0], -1, group_size).mean(axis=2)\n",
    "\n",
    "# Apply column averaging\n",
    "reduced_matrix = average_columns(activation_matrix, group_size=10)  # Shape will be (100, 12)\n",
    "\n",
    "# Save the reduced matrix\n",
    "output_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\reduced_reduced_activation_matrix_conv5_G.npy'\n",
    "np.save(output_path, reduced_matrix)\n",
    "\n",
    "print(f\"Reduced matrix saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bae8c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mname\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a736444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how much variance it explaines: \n",
    "total_variance_explained = svd.explained_variance_ratio_.sum()\n",
    "print(\"Total variance explained:\", total_variance_explained)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Number of Components')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
