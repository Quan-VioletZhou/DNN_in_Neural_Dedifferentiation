{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Subject Left Hemisphere Shape: (19072, 12)\n",
      "Example Subject Right Hemisphere Shape: (17995, 12)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to subject folders\n",
    "subject_dirs = glob.glob(r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\betafiles\\mindy*')\n",
    "\n",
    "# Dictionaries to store left and right hemisphere beta data for each subject\n",
    "left_hem_voxel_beta_values = {}\n",
    "right_hem_voxel_beta_values = {}\n",
    "\n",
    "# Load each hemisphere file for each subject\n",
    "for subject_dir in subject_dirs:\n",
    "    subject_id = os.path.basename(subject_dir)  # Extract 'mindyXXX' as subject ID\n",
    "    \n",
    "    # Load left hemisphere data\n",
    "    left_hem_path = os.path.join(subject_dir, 'LeftHemUnsm12conds.mat')\n",
    "    left_hem_data = scipy.io.loadmat(left_hem_path)\n",
    "    \n",
    "    # Stack columns from cond1 and cond2 to form a (19076, 12) matrix\n",
    "    left_hem_beta = np.hstack([left_hem_data['cond1'], left_hem_data['cond2']])\n",
    "    left_hem_voxel_beta_values[subject_id] = left_hem_beta  # Should now be (19076, 12)\n",
    "\n",
    "    # Load right hemisphere data\n",
    "    right_hem_path = os.path.join(subject_dir, 'RightHemUnsm12conds.mat')\n",
    "    right_hem_data = scipy.io.loadmat(right_hem_path)\n",
    "    \n",
    "    # Stack columns from cond1 and cond2 to form a (19076, 12) matrix\n",
    "    right_hem_beta = np.hstack([right_hem_data['cond1'], right_hem_data['cond2']])\n",
    "    right_hem_voxel_beta_values[subject_id] = right_hem_beta  # Should now be (19076, 12)\n",
    "\n",
    "# Optional: Check shapes for one subject to confirm successful loading\n",
    "example_subject = list(left_hem_voxel_beta_values.keys())[0]\n",
    "print(\"Example Subject Left Hemisphere Shape:\", left_hem_voxel_beta_values[example_subject].shape)\n",
    "print(\"Example Subject Right Hemisphere Shape:\", right_hem_voxel_beta_values[example_subject].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: shape (100, 12)\n",
      "rnorm1: shape (100, 12)\n",
      "pool1: shape (100, 12)\n",
      "conv2: shape (100, 12)\n",
      "rnorm2: shape (100, 12)\n",
      "pool2: shape (100, 12)\n",
      "conv3: shape (100, 12)\n",
      "conv4_W: shape (100, 12)\n",
      "conv5_W: shape (100, 12)\n",
      "pool5_W: shape (100, 12)\n",
      "conv4_G: shape (100, 12)\n",
      "conv5_G: shape (100, 12)\n",
      "pool5_G: shape (100, 12)\n"
     ]
    }
   ],
   "source": [
    "# prepare unit matrices for ridge regression\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define layer names as per your DNN structure\n",
    "layer_names = [ \n",
    "    'conv1', 'rnorm1', 'pool1', 'conv2', 'rnorm2', 'pool2',\n",
    "    'conv3', 'conv4_W', 'conv5_W', 'pool5_W', 'conv4_G', 'conv5_G', 'pool5_G'\n",
    "]\n",
    "\n",
    "# Path to .npy files for activations\n",
    "activation_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili'\n",
    "\n",
    "# Dictionary to store each layer's activation data\n",
    "activation_matrices = {}\n",
    "\n",
    "# Load each layer's .npy file and store it in activation_matrices\n",
    "for layer_name in layer_names:\n",
    "    # Adjust file name to match your format\n",
    "    file_path = os.path.join(activation_path, f'reduced_reduced_activation_matrix_{layer_name}.npy')\n",
    "    layer_data = np.load(file_path)\n",
    "    \n",
    "    # Ensure the data shape is (100, 12)\n",
    "    if layer_data.shape == (100, 120):\n",
    "        # Average every 10 columns to reduce to (100, 12)\n",
    "        layer_data = layer_data.reshape(100, 12, 10).mean(axis=2)\n",
    "    elif layer_data.shape != (100, 12):\n",
    "        raise ValueError(f\"Unexpected shape for {layer_name}: {layer_data.shape}\")\n",
    "    \n",
    "    # Store the processed data in the dictionary\n",
    "    activation_matrices[layer_name] = layer_data\n",
    "\n",
    "# Optional: Check the shapes to confirm all layers are correctly loaded\n",
    "for layer_name, data in activation_matrices.items():\n",
    "    print(f\"{layer_name}: shape {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m     ridge\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     38\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m ridge\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m---> 39\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     subject_fit_scores_right\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     41\u001b[0m fit_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m][layer_name]\u001b[38;5;241m.\u001b[39mappend(subject_fit_scores_right)\n",
      "File \u001b[1;32mc:\\Users\\quanz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\quanz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:992\u001b[0m, in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03mBest possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m-inf\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    989\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    990\u001b[0m     y_true, y_pred, multioutput\n\u001b[0;32m    991\u001b[0m )\n\u001b[1;32m--> 992\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    995\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR^2 score is not well-defined with less than two samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\quanz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03mChecks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\quanz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:276\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unpack_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\quanz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:125\u001b[0m, in \u001b[0;36m_unpack_tuple\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    121\u001b[0m     np\u001b[38;5;241m.\u001b[39msubtract(ary[\u001b[38;5;241m1\u001b[39m:], ary[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], result[l_begin:l_begin \u001b[38;5;241m+\u001b[39m l_diff])\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unpack_tuple\u001b[39m(x):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Unpacks one-element tuples for use as return values \"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this code is for ridge regression for all of the subjects:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Select a single subject ID (e.g., the first subject in your data)\n",
    "single_subject_id = list(left_hem_voxel_beta_values.keys())[0]\n",
    "\n",
    "# Assuming activation_matrices is a dictionary with each DNN layer's data (shape: (100, 12))\n",
    "# left_hem_voxel_beta_values and right_hem_voxel_beta_values contain the beta data for each hemisphere and subject\n",
    "\n",
    "# Dictionary to store fit scores for each hemisphere and layer\n",
    "fit_scores = {\n",
    "    \"left\": {layer_name: [] for layer_name in activation_matrices.keys()},\n",
    "    \"right\": {layer_name: [] for layer_name in activation_matrices.keys()},\n",
    "}\n",
    "\n",
    "# Run ridge regression for left and right hemispheres separately\n",
    "for layer_name, X in activation_matrices.items():  # X has shape (100, 12)\n",
    "    X = X.T  # Transpose to shape (12, 100), aligning with 12 conditions\n",
    "    \n",
    "    # Process left hemisphere data\n",
    "    for subject_id, left_hem_beta in left_hem_voxel_beta_values.items():\n",
    "        subject_fit_scores_left = []\n",
    "        for voxel_idx in range(left_hem_beta.shape[0]):\n",
    "            y = left_hem_beta[voxel_idx, :]  # Voxel responses across 12 conditions\n",
    "            ridge = Ridge(alpha=1.0)  # You can adjust alpha as needed\n",
    "            ridge.fit(X, y)\n",
    "            y_pred = ridge.predict(X)\n",
    "            score = r2_score(y, y_pred)  # R² score as the fit metric\n",
    "            subject_fit_scores_left.append(score)\n",
    "        fit_scores[\"left\"][layer_name].append(subject_fit_scores_left)\n",
    "\n",
    "    # Process right hemisphere data\n",
    "    for subject_id, right_hem_beta in right_hem_voxel_beta_values.items():\n",
    "        subject_fit_scores_right = []\n",
    "        for voxel_idx in range(right_hem_beta.shape[0]):\n",
    "            y = right_hem_beta[voxel_idx, :]  # Voxel responses across 12 conditions\n",
    "            ridge = Ridge(alpha=1.0)\n",
    "            ridge.fit(X, y)\n",
    "            y_pred = ridge.predict(X)\n",
    "            score = r2_score(y, y_pred)\n",
    "            subject_fit_scores_right.append(score)\n",
    "        fit_scores[\"right\"][layer_name].append(subject_fit_scores_right)\n",
    "\n",
    "# Convert scores to arrays for easier handling (optional)\n",
    "for hemi in fit_scores:\n",
    "    for layer_name in fit_scores[hemi]:\n",
    "        fit_scores[hemi][layer_name] = np.array(fit_scores[hemi][layer_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit scores for the single subject: {'left': {'conv1': array([0.97216182, 0.98573855, 0.979231  , ..., 0.97677105, 0.97426227,\n",
      "       0.96997001]), 'rnorm1': array([0.97222995, 0.98857945, 0.98383298, ..., 0.98896603, 0.98816909,\n",
      "       0.98547559]), 'pool1': array([0.96934983, 0.99272684, 0.98311599, ..., 0.99086871, 0.98945373,\n",
      "       0.98647828]), 'conv2': array([0.9701983 , 0.98634223, 0.97864708, ..., 0.97476117, 0.97219151,\n",
      "       0.96865383]), 'rnorm2': array([0.96828742, 0.98258644, 0.97625434, ..., 0.97156416, 0.96881062,\n",
      "       0.96513414]), 'pool2': array([0.97117667, 0.98025644, 0.98436181, ..., 0.97686648, 0.97201479,\n",
      "       0.96462737]), 'conv3': array([0.97332475, 0.97609066, 0.98643123, ..., 0.98132583, 0.9790628 ,\n",
      "       0.97228177]), 'conv4_W': array([0.97650152, 0.97286292, 0.98359165, ..., 0.97933301, 0.97634914,\n",
      "       0.96989551]), 'conv5_W': array([0.98262734, 0.98686039, 0.99015396, ..., 0.99132049, 0.99087276,\n",
      "       0.98813952]), 'pool5_W': array([0.98102989, 0.98928904, 0.98927168, ..., 0.99204657, 0.99002165,\n",
      "       0.98630896]), 'conv4_G': array([0.9716978 , 0.98563364, 0.98892052, ..., 0.98233266, 0.98172147,\n",
      "       0.97728444]), 'conv5_G': array([0.97072802, 0.98732097, 0.98977156, ..., 0.98285154, 0.98255943,\n",
      "       0.98054584]), 'pool5_G': array([0.96194135, 0.9839261 , 0.98853393, ..., 0.97410027, 0.97381124,\n",
      "       0.97140397])}, 'right': {'conv1': array([0.99057407, 0.96467537, 0.97179796, ..., 0.98517871, 0.98478628,\n",
      "       0.9824647 ]), 'rnorm1': array([0.99161431, 0.96747912, 0.98810648, ..., 0.98806135, 0.98892132,\n",
      "       0.98968271]), 'pool1': array([0.99398128, 0.96237322, 0.98542077, ..., 0.98523231, 0.9868117 ,\n",
      "       0.98895508]), 'conv2': array([0.99069366, 0.96435555, 0.96910252, ..., 0.98349161, 0.98369902,\n",
      "       0.98156548]), 'rnorm2': array([0.98598279, 0.963973  , 0.95942638, ..., 0.98585225, 0.98532183,\n",
      "       0.98267655]), 'pool2': array([0.98158236, 0.94150954, 0.95832667, ..., 0.98265182, 0.98293078,\n",
      "       0.98411068]), 'conv3': array([0.97329067, 0.94893079, 0.97498668, ..., 0.98319373, 0.98471221,\n",
      "       0.98771562]), 'conv4_W': array([0.97359609, 0.96058675, 0.96548612, ..., 0.98440101, 0.98462386,\n",
      "       0.98650706]), 'conv5_W': array([0.98994749, 0.97533653, 0.98586475, ..., 0.98877199, 0.98900531,\n",
      "       0.98951991]), 'pool5_W': array([0.98723382, 0.96294266, 0.97874225, ..., 0.98941657, 0.98980335,\n",
      "       0.9911923 ]), 'conv4_G': array([0.9836511 , 0.96586944, 0.98553279, ..., 0.98561922, 0.98591928,\n",
      "       0.98419026]), 'conv5_G': array([0.98410408, 0.97256328, 0.97390467, ..., 0.98334606, 0.98276781,\n",
      "       0.98165528]), 'pool5_G': array([0.98577482, 0.9668165 , 0.98073832, ..., 0.97760833, 0.97675035,\n",
      "       0.9764718 ])}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select a single subject ID (e.g., the first subject in your data)\n",
    "single_subject_id = list(left_hem_voxel_beta_values.keys())[0]\n",
    "\n",
    "# Dictionary to store fit scores for each hemisphere and layer for the single subject\n",
    "fit_scores_single_subject = {\n",
    "    \"left\": {layer_name: [] for layer_name in activation_matrices.keys()},\n",
    "    \"right\": {layer_name: [] for layer_name in activation_matrices.keys()},\n",
    "}\n",
    "\n",
    "# Run ridge regression for each layer and each hemisphere for the single subject\n",
    "for layer_name, X in activation_matrices.items():  # X has shape (100, 12)\n",
    "    X = X.T  # Transpose to shape (12, 100) for compatibility with voxel data\n",
    "\n",
    "    # Standardize the activations in X (if not already standardized)\n",
    "    scaler_X = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)  # Now X is standardized\n",
    "\n",
    "    # Process left hemisphere data for the single subject, voxel by voxel\n",
    "    left_hem_beta = left_hem_voxel_beta_values[single_subject_id]  # Shape: (19076, 12)\n",
    "    for voxel_idx in range(left_hem_beta.shape[0]):\n",
    "        y = left_hem_beta[voxel_idx, :]  # Voxel responses across 12 conditions\n",
    "\n",
    "        # Standardize y\n",
    "        scaler_y = StandardScaler()\n",
    "        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Ridge regression with scaled y\n",
    "        ridge = Ridge(alpha=10.0)\n",
    "        ridge.fit(X, y_scaled)\n",
    "\n",
    "        # Predict and inverse-transform to original scale\n",
    "        y_pred_scaled = ridge.predict(X)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate R² score on the original scale\n",
    "        score = r2_score(y, y_pred)\n",
    "        fit_scores_single_subject[\"left\"][layer_name].append(score)\n",
    "\n",
    "    # Process right hemisphere data for the single subject, voxel by voxel\n",
    "    right_hem_beta = right_hem_voxel_beta_values[single_subject_id]  # Shape: (19076, 12)\n",
    "    for voxel_idx in range(right_hem_beta.shape[0]):\n",
    "        y = right_hem_beta[voxel_idx, :]  # Voxel responses across 12 conditions\n",
    "\n",
    "        # Standardize y\n",
    "        scaler_y = StandardScaler()\n",
    "        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Ridge regression with scaled y\n",
    "        ridge = Ridge(alpha=10.0)\n",
    "        ridge.fit(X, y_scaled)\n",
    "\n",
    "        # Predict and inverse-transform to original scale\n",
    "        y_pred_scaled = ridge.predict(X)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate R² score on the original scale\n",
    "        score = r2_score(y, y_pred)\n",
    "        fit_scores_single_subject[\"right\"][layer_name].append(score)\n",
    "\n",
    "# Optional: Convert scores to arrays for easier handling\n",
    "for hemi in fit_scores_single_subject:\n",
    "    for layer_name in fit_scores_single_subject[hemi]:\n",
    "        fit_scores_single_subject[hemi][layer_name] = np.array(fit_scores_single_subject[hemi][layer_name])\n",
    "\n",
    "# Check scores for the single subject\n",
    "print(\"Fit scores for the single subject:\", fit_scores_single_subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare a list to store each score entry with details\n",
    "data_to_save = []\n",
    "\n",
    "# Loop through each hemisphere, layer, and voxel to structure the data\n",
    "for hemi, layers in fit_scores_single_subject.items():\n",
    "    for layer_name, scores in layers.items():\n",
    "        for voxel_idx, score in enumerate(scores):\n",
    "            data_to_save.append({\n",
    "                \"Hemisphere\": hemi,\n",
    "                \"Layer\": layer_name,\n",
    "                \"Voxel_Index\": voxel_idx,\n",
    "                \"R2_Score\": score\n",
    "            })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_scores = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save to CSV\n",
    "df_scores.to_csv(\"fit_scores_single_subject.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemisphere: left\n",
      "  Layer: conv1, Number of voxels: 19072\n",
      "  Layer: rnorm1, Number of voxels: 19072\n",
      "  Layer: pool1, Number of voxels: 19072\n",
      "  Layer: conv2, Number of voxels: 19072\n",
      "  Layer: rnorm2, Number of voxels: 19072\n",
      "  Layer: pool2, Number of voxels: 19072\n",
      "  Layer: conv3, Number of voxels: 19072\n",
      "  Layer: conv4_W, Number of voxels: 19072\n",
      "  Layer: conv5_W, Number of voxels: 19072\n",
      "  Layer: pool5_W, Number of voxels: 19072\n",
      "  Layer: conv4_G, Number of voxels: 19072\n",
      "  Layer: conv5_G, Number of voxels: 19072\n",
      "  Layer: pool5_G, Number of voxels: 19072\n",
      "Hemisphere: right\n",
      "  Layer: conv1, Number of voxels: 17995\n",
      "  Layer: rnorm1, Number of voxels: 17995\n",
      "  Layer: pool1, Number of voxels: 17995\n",
      "  Layer: conv2, Number of voxels: 17995\n",
      "  Layer: rnorm2, Number of voxels: 17995\n",
      "  Layer: pool2, Number of voxels: 17995\n",
      "  Layer: conv3, Number of voxels: 17995\n",
      "  Layer: conv4_W, Number of voxels: 17995\n",
      "  Layer: conv5_W, Number of voxels: 17995\n",
      "  Layer: pool5_W, Number of voxels: 17995\n",
      "  Layer: conv4_G, Number of voxels: 17995\n",
      "  Layer: conv5_G, Number of voxels: 17995\n",
      "  Layer: pool5_G, Number of voxels: 17995\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of R² score arrays within fit_scores_single_subject\n",
    "for hemi, layers in fit_scores_single_subject.items():\n",
    "    print(f\"Hemisphere: {hemi}\")\n",
    "    for layer_name, scores in layers.items():\n",
    "        print(f\"  Layer: {layer_name}, Number of voxels: {len(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
