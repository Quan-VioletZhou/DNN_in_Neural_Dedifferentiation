{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45709887-c1ec-4771-a80e-b8d3d957a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\quanz\\\\Documents\\\\UM\\\\Projects\\\\GLX_Project\\\\DNN\\\\kelletal2018-master\\\\kelletal2018-master\\\\network')\n",
    "from branched_network_class import branched_network\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "%pylab inline\n",
    "from pycochleagram import cochleagram as cgram \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907d5d7-8171-43dd-b128-d0c55e866bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(wav_f, sr, title):\n",
    "    # Define parameters\n",
    "    n, sampling_rate = 50, 16000\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "\n",
    "    # Create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "    \n",
    "    # Debugging statements\n",
    "    #print(\"Cochleagram shape:\", c_gram.shape)\n",
    "    #print(\"Min value in cochleagram:\", np.min(c_gram))\n",
    "    #print(\"Max value in cochleagram:\", np.max(c_gram))\n",
    "\n",
    "    # Avoiding log10 issues\n",
    "    # c_gram[c_gram <= 0] = 1e-10  # Set non-positive values to a small positive number\n",
    "\n",
    "    # Rescale to [0, 255]\n",
    "    # c_gram_rescaled = 255 * (1 - ((np.max(c_gram) - c_gram) / np.ptp(c_gram)))\n",
    "\n",
    "    # Check the shape of c_gram_rescaled\n",
    "    # print(\"Shape of c_gram_rescaled:\", c_gram_rescaled.shape)\n",
    "\n",
    "     # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "    \n",
    "    # plot_cochleagram(c_gram_reshape_2, title)\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "    \n",
    "    return c_gram_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb3a52-368b-4c1a-850e-5b3bdae6064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182661e-b93f-4443-8e32-bbe44695dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from branched_network_class import branched_network\n",
    "base_path = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\kelletal2018-master\\kelletal2018-master'\n",
    "\n",
    "# Save the current working directory\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "# Change to the directory where the weights file is located\n",
    "os.chdir(r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\kelletal2018-master\\kelletal2018-master')\n",
    "\n",
    "# Initialize the network object\n",
    "net_object = branched_network()\n",
    "print(\"Network object created successfully!\")\n",
    "\n",
    "\n",
    "# Load the keys for the model outputs\n",
    "# word_key = np.load('./demo_stim/logits_to_word_key.npy')  # Speech labels\n",
    "# music_key = np.load('./demo_stim/logits_to_genre_key.npy')  # Music genre labels\n",
    "\n",
    "# define the folder where we save all the segments we want to pass into DNN\n",
    "audio_folder = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\Speech\\Segments\\Ukranian'\n",
    "\n",
    "# List of layer names to extract\n",
    "layer_names = [\n",
    "    'conv1', 'rnorm1', 'pool1', 'conv2', 'rnorm2', 'pool2',\n",
    "    'conv3', 'conv4_W', 'conv5_W', 'pool5_W', 'conv4_G', 'conv5_G', 'pool5_G'\n",
    "]\n",
    "\n",
    "# Dictionary to store activations for each layer\n",
    "all_layer_activations = {layer: [] for layer in layer_names}\n",
    "\n",
    "# make a matrix to save the activation:\n",
    "    \n",
    "for filename in tqdm(os.listdir(audio_folder)):\n",
    "    if filename.endswith('.wav'):\n",
    "        sr, wav_f = wav.read(os.path.join(audio_folder, filename))\n",
    "        c_gram = generate_cochleagram(wav_f, sr, filename)\n",
    "\n",
    "        activations = net_object.get_activations(c_gram)  # Shape: (1, 86, 86, 96)\n",
    "\n",
    "     # Collect activations for each layer in the specified list\n",
    "        for layer_name in layer_names:\n",
    "            layer_activation = activations[layer_name][0]  # Remove batch dimension\n",
    "\n",
    "            # Flatten the spatial dimensions for the current segment\n",
    "            flattened_activation = layer_activation.reshape(-1)\n",
    "\n",
    "            # Append the flattened activation to the list for the current layer\n",
    "            all_layer_activations[layer_name].append(flattened_activation)\n",
    "\n",
    "# Set the output directory to the specified path\n",
    "output_directory = r'C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\Speech\\Segments\\Ukranian'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Stack activations for each layer across all segments and save as CSV\n",
    "for layer_name, activations_list in all_layer_activations.items():\n",
    "    # Stack activations along the segment axis to get an NxS matrix\n",
    "    stacked_activations = np.stack(activations_list, axis=1)  # Shape: (N, S)\n",
    "\n",
    "    # Create a DataFrame for easier inspection and saving\n",
    "    df = pd.DataFrame(stacked_activations, columns=[f'Segment_{i+1}' for i in range(stacked_activations.shape[1])])\n",
    "    df.index = [f'Unit_{j+1}' for j in range(stacked_activations.shape[0])]  # Label rows by unit\n",
    "\n",
    "    # Calculate the average activation across segments\n",
    "    avg_activations = stacked_activations.mean(axis=1)\n",
    "    df['Average_Activation'] = avg_activations  # Add as a new column\n",
    "\n",
    "    # Save the matrix with the average column to CSV in the specified output directory\n",
    "    output_csv = os.path.join(output_directory, f'{layer_name}_activations_with_avg.csv')\n",
    "    df.to_csv(output_csv)\n",
    "    print(f\"Saved activation matrix with averages for {layer_name} to {output_csv}\")\n",
    "\n",
    "    # Save the average activation as a separate CSV (optional)\n",
    "    avg_df = pd.DataFrame(avg_activations, columns=['Average_Activation'])\n",
    "    avg_output_csv = os.path.join(output_directory, f'{layer_name}_average_activation.csv')\n",
    "    avg_df.to_csv(avg_output_csv, index=False)\n",
    "    print(f\"Saved average activation vector for {layer_name} to {avg_output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d713f-9e79-4555-90b0-5b04cde259cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d947082-5de3-4ddd-8814-449c297d2a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined music data to C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\\conv1_speech_combined_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths and stimuli for the music category\n",
    "# music_stimuli = [\"Cello\", \"Guitar\", \"Saxophone\", \"Flute\", \"Piano\", \"Violin\"]\n",
    "speech_stimuli = [\"Ukranian\", \"Macedonian\", \"Sinhalese\", \"Persian\", \"Marathi\",\"Swahili\"]\n",
    "base_path = r\"C:\\Users\\quanz\\Documents\\UM\\Projects\\GLX_Project\\DNN\\MiND_Stimili\"\n",
    "layer_name = 'conv1'\n",
    "\n",
    "# Function to process a single stimulus file by removing specified columns/rows\n",
    "def process_stimulus(file_path):\n",
    "    # Load the data without headers, skip first row, remove first and last columns\n",
    "    df = pd.read_csv(file_path, skiprows=1, header=None).iloc[:, 1:-1]\n",
    "    return df\n",
    "\n",
    "# Combine data for all music stimuli, extending columns\n",
    "combined_music_data = []\n",
    "\n",
    "for stimulus in speech_stimuli:\n",
    "    file_path = os.path.join(base_path, \"Speech\", \"Segments\", stimulus, f\"{layer_name}_activations_with_avg.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        processed_df = process_stimulus(file_path)\n",
    "        combined_music_data.append(processed_df)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Concatenate all processed music data horizontally (extend columns)\n",
    "final_music_matrix = pd.concat(combined_music_data, axis=1)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "output_path = os.path.join(base_path, f\"{layer_name}_speech_combined_matrix.csv\")\n",
    "final_music_matrix.to_csv(output_path, index=False)\n",
    "print(f\"Saved combined music data to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4424e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(710016, 60)\n",
      "(710015, 10)\n"
     ]
    }
   ],
   "source": [
    "print(final_music_matrix.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7f851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
